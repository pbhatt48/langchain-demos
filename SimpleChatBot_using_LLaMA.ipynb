{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMzpyIpZIYN5w7WEfuh5gE6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9b55450443c6429a8e27524c02a5723a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55417a6dddee489ca667e81d1bdd849c",
              "IPY_MODEL_be5d53c6d8c142b6a58287c1e2f9eacc",
              "IPY_MODEL_4c8cad19b18046d2aedb4e51a0fb4a27"
            ],
            "layout": "IPY_MODEL_6927887321ca44379736015f79d6a9b1"
          }
        },
        "55417a6dddee489ca667e81d1bdd849c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed5f47c849fe4fc8a23ad33e11e5320b",
            "placeholder": "​",
            "style": "IPY_MODEL_bde94f9e8b83467aa027223ab0c308cc",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "be5d53c6d8c142b6a58287c1e2f9eacc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7edce98a77c742c98d8c2a494c0051c5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f69ca8d7c8e4d9a8459b27743dcd802",
            "value": 2
          }
        },
        "4c8cad19b18046d2aedb4e51a0fb4a27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d47ef1d6a7548759776d888e45aa3e8",
            "placeholder": "​",
            "style": "IPY_MODEL_9c29825182ad4a8f996fe043bc839349",
            "value": " 2/2 [01:06&lt;00:00, 66.42s/it]"
          }
        },
        "6927887321ca44379736015f79d6a9b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed5f47c849fe4fc8a23ad33e11e5320b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bde94f9e8b83467aa027223ab0c308cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7edce98a77c742c98d8c2a494c0051c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f69ca8d7c8e4d9a8459b27743dcd802": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d47ef1d6a7548759776d888e45aa3e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c29825182ad4a8f996fe043bc839349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pbhatt48/langchain-demos/blob/main/SimpleChatBot_using_LLaMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RRk-PUNeJ5j8"
      },
      "outputs": [],
      "source": [
        "## install packages\n",
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "subprocess.run([\"huggingface-cli\", \"login\", \"--token\", \"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\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxu7aPXhKSQj",
        "outputId": "a653b5c6-901e-4719-817e-2245e2068de4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['huggingface-cli', 'login', '--token', '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'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "MuUlFGqIKcol"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                                        use_auth_token=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACuhWC9eKgv8",
        "outputId": "ec4cb556-5285-4c31-ab4f-68e16a689a6c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:631: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                                           device_map='auto',\n",
        "                                           torch_dtype=torch.float16,\n",
        "                                           use_auth_token=True,\n",
        "                                           max_memory={0: \"10GiB\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "9b55450443c6429a8e27524c02a5723a",
            "55417a6dddee489ca667e81d1bdd849c",
            "be5d53c6d8c142b6a58287c1e2f9eacc",
            "4c8cad19b18046d2aedb4e51a0fb4a27",
            "6927887321ca44379736015f79d6a9b1",
            "ed5f47c849fe4fc8a23ad33e11e5320b",
            "bde94f9e8b83467aa027223ab0c308cc",
            "7edce98a77c742c98d8c2a494c0051c5",
            "2f69ca8d7c8e4d9a8459b27743dcd802",
            "1d47ef1d6a7548759776d888e45aa3e8",
            "9c29825182ad4a8f996fe043bc839349"
          ]
        },
        "id": "cwG490-bgC4g",
        "outputId": "71d42aeb-5e3a-4553-cb20-b8bce3ab6921"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:460: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b55450443c6429a8e27524c02a5723a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:373: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline=transformers.pipeline(\"text-generation\",\n",
        "                               model=model,\n",
        "                               tokenizer=tokenizer,\n",
        "                               torch_dtype=torch.bfloat16,\n",
        "                               device_map=\"auto\",\n",
        "                               # Maximum number of tokens to generate, a word is generally 2-3 tokens (minimum: 1)\n",
        "                               max_new_tokens=512,\n",
        "                               #Minimum number of tokens to generate. To disable this set it to -1. A word is generally 2-3 tokens (minimum 1)\n",
        "                               min_new_tokens=-1,\n",
        "                               #Adjusts the randomness of output\n",
        "                               temperature=0.75,\n",
        "                               do_sample=True,\n",
        "                               top_k=30,\n",
        "                               num_return_sequences=1,\n",
        "                               eos_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "xClEOEvYKt3T"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "COVezOLLKzTo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8031b2c-9b94-4f1d-a2e8-1d3e9380bbb1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Aug 24 03:10:17 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    27W /  70W |  10461MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm=HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature':0})"
      ],
      "metadata": {
        "id": "uHrnt3bLK5z_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define instruction token and system token\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<<SYS>>>\\n\", \"\\n<<<SYS>>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\" You are a helpful AI assistant. \"\"\"\n",
        "instruction = \"Write an essay on the topic: \\n {text}\"\n",
        "SYSTEM_PROMPT = B_SYS + DEFAULT_SYSTEM_PROMPT + E_SYS\n",
        "template = B_INST + DEFAULT_SYSTEM_PROMPT + instruction + E_INST\n",
        "print(template)"
      ],
      "metadata": {
        "id": "ncMy2AlGK6Fl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce11b21a-ce48-413e-af2a-273d439bc669"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST] You are a helpful AI assistant. Write an essay on the topic: \n",
            " {text}[/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets create a prompt\n",
        "text = \"Henry Ford and Model T\"\n",
        "prompt = PromptTemplate(input_variables=['text'], template=template)\n",
        "LLM_Chain = LLMChain(llm=llm, prompt=prompt)\n",
        "LLM_Chain.run(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "TqY9ym7laET0",
        "outputId": "dad33aa3-4d1c-427e-c170-f8326e094752"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  Henry Ford was an American industrialist, inventor, and entrepreneur who is best known for revolutionizing the automotive industry with his innovative and affordable Model T car. Ford's impact on society was immense, as he transformed the way people lived, worked, and traveled. In this essay, we will explore Ford's life, his invention of the Model T, and its impact on society.\\nEarly Life and Career\\nHenry Ford was born on July 30, 1863, in Dearborn, Michigan, to a family of Irish immigrants. From an early age, Ford showed a passion for inventing and tinkering with machines. After completing his education, he worked as an apprentice in a machine shop and later started his own company, the Ford Motor Company, in 1903.\\nFord's first major invention was the Quadricycle, a gasoline-powered vehicle that he built in his own garage. Although the Quadricycle was not a commercial success, it demonstrated Ford's ability to innovate and think outside the box. In 1908, Ford introduced the Model T, which would become one of the most influential cars in history.\\nThe Model T\\nThe Model T was designed to be affordable, reliable, and easy to maintain. It was powered by a four-cylinder engine and featured a lightweight body made of steel and wood. The car was produced on an assembly line, which allowed Ford to mass-produce the vehicles at a lower cost than his competitors. The Model T was an instant success, selling over 15 million units between 1908 and 1927.\\nImpact on Society\\nThe Model T had a profound impact on society, transforming the way people lived, worked, and traveled. Before the Model T, cars were expensive and unaffordable for most people. The Model T made cars accessible to the average person, allowing them to travel longer distances and explore new places. The car also had a significant impact on the agricultural industry, as farmers were able to transport their goods more efficiently and sell them in distant markets.\\nThe Model T also changed the way people worked. With the ability to travel faster and farther, people were able to commute to work and conduct business in new ways. The\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Example 2\n",
        "CUSTOM_SYSTEM_PROMPT = \"You are an efficient translator.\"\n",
        "instruction = \"Convert the text from english to french language: \\n {text}\"\n",
        "SYSTEM_PROMPT = B_SYS + CUSTOM_SYSTEM_PROMPT + E_SYS\n",
        "template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "print(template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s99bBQq2bG0A",
        "outputId": "ad6522c6-7828-4463-d461-0e566fd49ca5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]<<<SYS>>>\n",
            "You are an efficient translator.\n",
            "<<<SYS>>>\n",
            "\n",
            "Convert the text from english to french language: \n",
            " {text}[/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am trying to learn Frech. Please teach me french in easy way. Thank you!\"\n",
        "prompt = PromptTemplate(input_variables=['text'], template=template)\n",
        "LLM_Chain = LLMChain(llm=llm, prompt=prompt)\n",
        "LLM_Chain.run(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "3TzvOcuXkDf5",
        "outputId": "392e10ac-17df-4a7c-e141-ba9e517f9680"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  Sure, I\\'d be happy to help you learn French! Here\\'s the translation of your message from English to French:\\nJe suis en train d\\'apprendre le français. Pouvez-vous me enseigner le français de manière facile? Merci!\\n\\nHere\\'s a breakdown of the translation:\\n\\n* \"Je suis en train d\\'apprendre le français\" means \"I am trying to learn French.\"\\n* \"Pouvez-vous me enseigner le français de manière facile?\" means \"Can you teach me French in an easy way?\"\\n* \"Merci\" is a common French phrase used to express gratitude or appreciation.\\nI hope this helps you get started with your French learning journey! If you have any questions or need further assistance, feel free to ask.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Summary example\n",
        "CUSTOM_SYSTEM_PROMPT = \"You are good at summarizing text.\"\n",
        "instruction = \"Summarize the content into 300 words for the following text : \\n {text}\"\n",
        "SYSTEM_PROMPT = B_SYS + CUSTOM_SYSTEM_PROMPT +E_SYS\n",
        "template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "print(template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5bRTiialF4d",
        "outputId": "e93cce03-d3ed-4f47-8ffb-0acdf1aab4ac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]<<<SYS>>>\n",
            "You are good at summarizing text.\n",
            "<<<SYS>>>\n",
            "\n",
            "Summarize the content into 300 words for the following text : \n",
            " {text}[/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "1. Aristotle’s Life\n",
        "Born in 384 B.C.E. in the Macedonian region of northeastern Greece in the small city of Stagira (whence the moniker ‘the Stagirite’, which one still occasionally encounters in Aristotelian scholarship), Aristotle was sent to Athens at about the age of seventeen to study in Plato’s Academy, then a pre-eminent place of learning in the Greek world. Once in Athens, Aristotle remained associated with the Academy until Plato’s death in 347, at which time he left for Assos, in Asia Minor, on the northwest coast of present-day Turkey. There he continued the philosophical activity he had begun in the Academy, but in all likelihood also began to expand his researches into marine biology. He remained at Assos for approximately three years, when, evidently upon the death of his host Hermeias, a friend and former Academic who had been the ruler of Assos, Aristotle moved to the nearby coastal island of Lesbos. There he continued his philosophical and empirical researches for an additional two years, working in conjunction with Theophrastus, a native of Lesbos who was also reported in antiquity to have been associated with Plato’s Academy. While in Lesbos, Aristotle married Pythias, the niece of Hermeias, with whom he had a daughter, also named Pythias.\n",
        "\n",
        "In 343, upon the request of Philip, the king of Macedon, Aristotle left Lesbos for Pella, the Macedonian capital, in order to tutor the king’s thirteen-year-old son, Alexander—the boy who was eventually to become Alexander the Great. Although speculation concerning Aristotle’s influence upon the developing Alexander has proven irresistible to historians, in fact little concrete is known about their interaction. On the balance, it seems reasonable to conclude that some tuition took place, but that it lasted only two or three years, when Alexander was aged from thirteen to fifteen. By fifteen, Alexander was apparently already serving as a deputy military commander for his father, a circumstance undermining, if inconclusively, the judgment of those historians who conjecture a longer period of tuition. Be that as it may, some suppose that their association lasted as long as eight years.\n",
        "\n",
        "It is difficult to rule out that possibility decisively, since little is known about the period of Aristotle’s life from 341–335. He evidently remained a further five years in Stagira or Macedon before returning to Athens for the second and final time, in 335. In Athens, Aristotle set up his own school in a public exercise area dedicated to the god Apollo Lykeios, whence its name, the Lyceum. Those affiliated with Aristotle’s school later came to be called Peripatetics, probably because of the existence of an ambulatory (peripatos) on the school’s property adjacent to the exercise ground. Members of the Lyceum conducted research into a wide range of subjects, all of which were of interest to Aristotle himself: botany, biology, logic, music, mathematics, astronomy, medicine, cosmology, physics, the history of philosophy, metaphysics, psychology, ethics, theology, rhetoric, political history, government and political theory, and the arts. In all these areas, the Lyceum collected manuscripts, thereby, according to some ancient accounts, assembling the first great library of antiquity.\n",
        "\n",
        "During this period, Aristotle’s wife, Pythias, died and he developed a new relationship with Herpyllis, perhaps like him a native of Stagira, though her origins are disputed, as is the question of her exact relationship to Aristotle. Some suppose that she was merely his slave; others infer from the provisions of Aristotle’s will that she was a freed woman and likely his wife at the time of his death. In any event, they had children together, including a son, Nicomachus, named for Aristotle’s father and after whom his Nicomachean Ethics is presumably named.\n",
        "\n",
        "After thirteen years in Athens, Aristotle once again found cause to retire from the city, in 323. Probably his departure was occasioned by a resurgence of the always-simmering anti-Macedonian sentiment in Athens, which was free to come to the boil after Alexander succumbed to disease in Babylon during that same year. Because of his connections to Macedon, Aristotle reasonably feared for his safety and left Athens, remarking, as an oft-repeated ancient tale would tell it, that he saw no reason to permit Athens to sin twice against philosophy. He withdrew directly to Chalcis, on Euboea, an island off the Attic coast, and died there of natural causes the following year, in 322.[3]\n",
        "\n",
        "2. The Aristotelian Corpus: Character and Primary Divisions\n",
        "Aristotle’s writings tend to present formidable difficulties to his novice readers. To begin, he makes heavy use of unexplained technical terminology, and his sentence structure can at times prove frustrating. Further, on occasion a chapter or even a full treatise coming down to us under his name appears haphazardly organized, if organized at all; indeed, in several cases, scholars dispute whether a continuous treatise currently arranged under a single title was ever intended by Aristotle to be published in its present form or was rather stitched together by some later editor employing whatever principles of organization he deemed suitable.[4] This helps explain why students who turn to Aristotle after first being introduced to the supple and mellifluous prose on display in Plato’s dialogues often find the experience frustrating. Aristotle’s prose requires some acclimatization.\n",
        "\n",
        "All the more puzzling, then, is Cicero’s observation that if Plato’s prose was silver, Aristotle’s was a flowing river of gold (Ac. Pr. 38.119, cf. Top. 1.3, De or. 1.2.49). Cicero was arguably the greatest prose stylist of Latin and was also without question an accomplished and fair-minded critic of the prose styles of others writing in both Latin and Greek. We must assume, then, that Cicero had before him works of Aristotle other than those we possess. In fact, we know that Aristotle wrote dialogues, presumably while still in the Academy, and in their few surviving remnants we are afforded a glimpse of the style Cicero describes. In most of what we possess, unfortunately, we find work of a much less polished character. Rather, Aristotle’s extant works read like what they very probably are: lecture notes, drafts first written and then reworked, ongoing records of continuing investigations, and, generally speaking, in-house compilations intended not for a general audience but for an inner circle of auditors. These are to be contrasted with the “exoteric” writings Aristotle sometimes mentions, his more graceful compositions intended for a wider audience (Pol. 1278b30; EE 1217b22, 1218b34). Unfortunately, then, we are left for the most part, though certainly not entirely, with unfinished works in progress rather than with finished and polished productions. Still, many of those who persist with Aristotle come to appreciate the unembellished directness of his style.\n",
        "\n",
        "More importantly, the unvarnished condition of Aristotle’s surviving treatises does not hamper our ability to come to grips with their philosophical content. His thirty-one surviving works (that is, those contained in the “Corpus Aristotelicum” of our medieval manuscripts that are judged to be authentic) all contain recognizably Aristotelian doctrine; and most of these contain theses whose basic purport is clear, even where matters of detail and nuance are subject to exegetical controversy.\n",
        "\n",
        "These works may be categorized in terms of the intuitive organizational principles preferred by Aristotle. He refers to the branches of learning as “sciences” (epistêmai), best regarded as organized bodies of learning completed for presentation rather than as ongoing records of empirical researches. Moreover, again in his terminology, natural sciences such as physics are but one branch of theoretical science, which comprises both empirical and non-empirical pursuits. He distinguishes theoretical science from more practically oriented studies, some of which concern human conduct and others of which focus on the productive crafts. Thus, the Aristotelian sciences divide into three: (i) theoretical, (ii) practical, and (iii) productive. The principles of division are straightforward: theoretical science seeks knowledge for its own sake; practical science concerns conduct and goodness in action, both individual and societal; and productive science aims at the creation of beautiful or useful objects (Top. 145a15–16; Phys. 192b8–12; DC 298a27–32, DA 403a27–b2; Met. 1025b25, 1026a18–19, 1064a16–19, b1–3; EN 1139a26–28, 1141b29–32).\n",
        "\n",
        "(i) The theoretical sciences include prominently what Aristotle calls first philosophy, or metaphysics as we now call it, but also mathematics, and physics, or natural philosophy. Physics studies the natural universe as a whole, and tends in Aristotle’s hands to concentrate on conceptual puzzles pertaining to nature rather than on empirical research; but it reaches further, so that it includes also a theory of causal explanation and finally even a proof of an unmoved mover thought to be the first and final cause of all motion. Many of the puzzles of primary concern to Aristotle have proven perennially attractive to philosophers, mathematicians, and theoretically inclined natural scientists. They include, as a small sample, Zeno’s paradoxes of motion, puzzles about time, the nature of place, and difficulties encountered in thought about the infinite.\n",
        "\n",
        "Natural philosophy also incorporates the special sciences, including biology, botany, and astronomical theory. Most contemporary critics think that Aristotle treats psychology as a sub-branch of natural philosophy, because he regards the soul (psuchê) as the basic principle of life, including all animal and plant life. In fact, however, the evidence for this conclusion is inconclusive at best. It is instructive to note that earlier periods of Aristotelian scholarship thought this controversial, so that, for instance, even something as innocuous-sounding as the question of the proper home of psychology in Aristotle’s division of the sciences ignited a multi-decade debate in the Renaissance.[5]\n",
        "\n",
        "(ii) Practical sciences are less contentious, at least as regards their range. These deal with conduct and action, both individual and societal. Practical science thus contrasts with theoretical science, which seeks knowledge for its own sake, and, less obviously, with the productive sciences, which deal with the creation of products external to sciences themselves. Both politics and ethics fall under this branch.\n",
        "\n",
        "(iii) Finally, then, the productive sciences are mainly crafts aimed at the production of artefacts, or of human productions more broadly construed. The productive sciences include, among others, ship-building, agriculture, and medicine, but also the arts of music, theatre, and dance. Another form of productive science is rhetoric, which treats the principles of speech-making appropriate to various forensic and persuasive settings, including centrally political assemblies.\n",
        "\n",
        "Significantly, Aristotle’s tri-fold division of the sciences makes no mention of logic. Although he did not use the word ‘logic’ in our sense of the term, Aristotle in fact developed the first formalized system of logic and valid inference. In Aristotle’s framework—although he is nowhere explicit about this—logic belongs to no one science, but rather formulates the principles of correct argumentation suitable to all areas of inquiry in common. It systematizes the principles licensing acceptable inference, and helps to highlight at an abstract level seductive patterns of incorrect inference to be avoided by anyone with a primary interest in truth. So, alongside his more technical work in logic and logical theory, Aristotle investigates informal styles of argumentation and seeks to expose common patterns of fallacious reasoning.\n",
        "\n",
        "Aristotle’s investigations into logic and the forms of argumentation make up part of the group of works coming down to us from the Middle Ages under the heading the Organon (organon = tool in Greek). Although not so characterized in these terms by Aristotle, the name is apt, so long as it is borne in mind that intellectual inquiry requires a broad range of tools. Thus, in addition to logic and argumentation (treated primarily in the Prior Analytics and Topics), the works included in the Organon deal with category theory, the doctrine of propositions and terms, the structure of scientific theory, and to some extent the basic principles of epistemology.\n",
        "\n",
        "When we slot Aristotle’s most important surviving authentic works into this scheme, we end up with the following basic divisions of his major writings:\n",
        "\n",
        "Organon\n",
        "Categories (Cat.)\n",
        "De Interpretatione (DI) [On Interpretation]\n",
        "Prior Analytics (APr)\n",
        "Posterior Analytics (APo)\n",
        "Topics (Top.)\n",
        "Sophistical Refutations (SE)\n",
        "Theoretical Sciences\n",
        "Physics (Phys.)\n",
        "Generation and Corruption (Gen. et Corr.)\n",
        "De Caelo (DC) [On the Heavens]\n",
        "Metaphysics (Met.)\n",
        "De Anima (DA) [On the Soul]\n",
        "Parva Naturalia (PN) [Brief Natural Treatises]\n",
        "History of Animals (HA)\n",
        "Parts of Animals (PA)\n",
        "Movement of Animals (MA)\n",
        "Meteorology (Meteor.)\n",
        "Progression of Animals (IA)\n",
        "Generation of Animals (GA)\n",
        "Practical Sciences\n",
        "Nicomachean Ethics (EN)\n",
        "Eudemian Ethics (EE)\n",
        "Magna Moralia (MM) [Great Ethics]\n",
        "Politics (Pol.)\n",
        "Productive Science\n",
        "Rhetoric (Rhet.)\n",
        "Poetics (Poet.)\n",
        "The titles in this list are those in most common use today in English-language scholarship, followed by standard abbreviations in parentheses. For no discernible reason, Latin titles are customarily employed in some cases, English in others. Where Latin titles are in general use, English equivalents are given in square brackets.\n",
        "\n",
        "3. Phainomena and the Endoxic Method\n",
        "Aristotle’s basic approach to philosophy is best grasped initially by way of contrast. Whereas Descartes seeks to place philosophy and science on firm foundations by subjecting all knowledge claims to a searing methodological doubt, Aristotle begins with the conviction that our perceptual and cognitive faculties are basically dependable, that they for the most part put us into direct contact with the features and divisions of our world, and that we need not dally with sceptical postures before engaging in substantive philosophy. Accordingly, he proceeds in all areas of inquiry in the manner of a modern-day natural scientist, who takes it for granted that progress follows the assiduous application of a well-trained mind and so, when presented with a problem, simply goes to work. When he goes to work, Aristotle begins by considering how the world appears, reflecting on the puzzles those appearances throw up, and reviewing what has been said about those puzzles to date. These methods comprise his twin appeals to phainomena and the endoxic method.\n",
        "\n",
        "These two methods reflect in different ways Aristotle’s deepest motivations for doing philosophy in the first place. “Human beings began to do philosophy,” he says, “even as they do now, because of wonder, at first because they wondered about the strange things right in front of them, and then later, advancing little by little, because they came to find greater things puzzling” (Met. 982b12). Human beings philosophize, according to Aristotle, because they find aspects of their experience puzzling. The sorts of puzzles we encounter in thinking about the universe and our place within it—aporiai, in Aristotle’s terminology—tax our understanding and induce us to philosophize.\n",
        "\n",
        "According to Aristotle, it behooves us to begin philosophizing by laying out the phainomena, the appearances, or, more fully, things appearing to be the case, and then also collecting the endoxa, the credible opinions handed down regarding matters we find puzzling. As a typical example, in a passage of his Nicomachean Ethics, Aristotle confronts a puzzle of human conduct, the fact that we are apparently sometimes akratic or weak-willed. When introducing this puzzle, Aristotle pauses to reflect upon a precept governing his approach to many areas of inquiry:\n",
        "\n",
        "As in other cases, we must set out the appearances (phainomena) and run through all the puzzles regarding them. In this way we must prove the credible opinions (endoxa) about these sorts of experiences—ideally, all the credible opinions, but if not all, then most of them, those which are the most important. For if the objections are answered and the credible opinions remain, we shall have an adequate proof. (EN 1145b2–7)\n",
        "Scholars dispute concerning the degree to which Aristotle regards himself as beholden to the credible opinions (endoxa) he recounts and the basic appearances (phainomena) to which he appeals.[6] Of course, since the endoxa will sometimes conflict with one another, often precisely because the phainomena generate aporiai, or puzzles, it is not always possible to respect them in their entirety. So, as a group they must be re-interpreted and systematized, and, where that does not suffice, some must be rejected outright. It is in any case abundantly clear that Aristotle is willing to abandon some or all of the endoxa and phainomena whenever science or philosophy demands that he do so (Met. 1073b36, 1074b6; PA 644b5; EN 1145b2–30).\n",
        "\n",
        "Still, his attitude towards phainomena does betray a preference to conserve as many appearances as is practicable in a given domain—not because the appearances are unassailably accurate, but rather because, as he supposes, appearances tend to track the truth. We are outfitted with sense organs and powers of mind so structured as to put us into contact with the world and thus to provide us with data regarding its basic constituents and divisions. While our faculties are not infallible, neither are they systematically deceptive or misdirecting. Since philosophy’s aim is truth and much of what appears to us proves upon analysis to be correct, phainomena provide both an impetus to philosophize and a check on some of its more extravagant impulses.\n",
        "\n",
        "Of course, it is not always clear what constitutes a phainomenon; still less is it clear which phainomenon is to be respected in the face of bona fide disagreement. This is in part why Aristotle endorses his second and related methodological precept, that we ought to begin philosophical discussions by collecting the most stable and entrenched opinions regarding the topic of inquiry handed down to us by our predecessors. Aristotle’s term for these privileged views, endoxa, is variously rendered as ‘reputable opinions’, ‘credible opinions’, ‘entrenched beliefs’, ‘credible beliefs’, or ‘common beliefs’. Each of these translations captures at least part of what Aristotle intends with this word, but it is important to appreciate that it is a fairly technical term for him. An endoxon is the sort of opinion we spontaneously regard as reputable or worthy of respect, even if upon reflection we may come to question its veracity. (Aristotle appropriates this term from ordinary Greek, in which an endoxos is a notable or honourable man, a man of high repute whom we would spontaneously respect—though we might, of course, upon closer inspection, find cause to criticize him.) As he explains his use of the term, endoxa are widely shared opinions, often ultimately issuing from those we esteem most: ‘Endoxa are those opinions accepted by everyone, or by the majority, or by the wise—and among the wise, by all or most of them, or by those who are the most notable and having the highest reputation’ (Top. 100b21–23). Endoxa play a special role in Aristotelian philosophy in part because they form a significant sub-class of phainomena (EN 1154b3–8): because they are the privileged opinions we find ourselves unreflectively endorsing and reaffirming after some reflection, they themselves come to qualify as appearances to be preserved where possible.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ItdTuYWstXQV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"\"\"\n",
        "Aristotle’s Life\n",
        "Born in 384 B.C.E. in the Macedonian region of northeastern Greece in the small city of Stagira (whence the moniker ‘the Stagirite’, which one still occasionally encounters in Aristotelian scholarship), Aristotle was sent to Athens at about the age of seventeen to study in Plato’s Academy, then a pre-eminent place of learning in the Greek world. Once in Athens, Aristotle remained associated with the Academy until Plato’s death in 347, at which time he left for Assos, in Asia Minor, on the northwest coast of present-day Turkey. There he continued the philosophical activity he had begun in the Academy, but in all likelihood also began to expand his researches into marine biology. He remained at Assos for approximately three years, when, evidently upon the death of his host Hermeias, a friend and former Academic who had been the ruler of Assos, Aristotle moved to the nearby coastal island of Lesbos. There he continued his philosophical and empirical researches for an additional two years, working in conjunction with Theophrastus, a native of Lesbos who was also reported in antiquity to have been associated with Plato’s Academy. While in Lesbos, Aristotle married Pythias, the niece of Hermeias, with whom he had a daughter, also named Pythias.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LZ6gOGKyuhAd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(input_variables=['text'], template=template)\n",
        "LLM_Chain = LLMChain(llm=llm, prompt=prompt)\n",
        "output = LLM_Chain.run(text)\n",
        "import textwrap\n",
        "wrapped_text = textwrap.fill(output, width=100)\n",
        "print(wrapped_text +'\\n\\n')"
      ],
      "metadata": {
        "id": "jNti2MNWt6eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####Chat\n",
        "\n",
        "##Creating a simple chatbot\n"
      ],
      "metadata": {
        "id": "2Wky5uU949Sz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "instruction = \"Chat History: \\n {chat_history} \\n User: \\n {user_input}\"\n",
        "CUSTOM_SYSTEM_PROMPT = \"You are a helpfule assistant!\"\n"
      ],
      "metadata": {
        "id": "S4KKIcxulwHG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define instruction token and system token\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<<SYS>>>\\n\", \"\\n<<<SYS>>>\\n\\n\"\n",
        "\n",
        "SYSTEM_PROMPT = B_SYS + CUSTOM_SYSTEM_PROMPT + E_SYS\n",
        "template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "print(template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhRYcFpHmGBN",
        "outputId": "87d934f1-ad7f-497c-e075-ddb78123454e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]<<<SYS>>>\n",
            "You are a helpfule assistant!\n",
            "<<<SYS>>>\n",
            "\n",
            "Chat History: \n",
            " {chat_history} \n",
            " User: \n",
            " {user_input}[/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(input_variables=['chat_history', 'user_input'], template= template)\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
      ],
      "metadata": {
        "id": "VE_gHBQqmkmz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt, memory=memory, verbose=True)\n",
        "chain.run(user_input=\"Hi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "hsvZgiGGm9-v",
        "outputId": "7a4c6561-1669-488d-8750-01cfa6931912"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<<SYS>>>\n",
            "You are a helpfule assistant!\n",
            "<<<SYS>>>\n",
            "\n",
            "Chat History: \n",
            "  \n",
            " User: \n",
            " Hi[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  Hello! It's nice to meet you. Is there something I can help you with? 😊\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(user_input=\"Who is the president of USA?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "-ZW1WfNinIv2",
        "outputId": "8c3c4257-24b7-4f2a-cec0-343f4bd2679e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<<SYS>>>\n",
            "You are a helpfule assistant!\n",
            "<<<SYS>>>\n",
            "\n",
            "Chat History: \n",
            " Human: Hi\n",
            "AI:   Hello! It's nice to meet you. Is there something I can help you with? 😊 \n",
            " User: \n",
            " Who is the president of USA?[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  AI: Great, I'm glad you asked! The current President of the United States is Joe Biden. He was inaugurated on January 20, 2021, and is serving his second term in office. 🇺🇸\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(user_input=\"What political party is he affiliated to?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "krpveTvdnT8k",
        "outputId": "f1960d38-a834-4f4c-fa3a-9b6fe3cc2274"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<<SYS>>>\n",
            "You are a helpfule assistant!\n",
            "<<<SYS>>>\n",
            "\n",
            "Chat History: \n",
            " Human: Hi\n",
            "AI:   Hello! It's nice to meet you. Is there something I can help you with? 😊\n",
            "Human: Who is the president of USA?\n",
            "AI:   AI: Great, I'm glad you asked! The current President of the United States is Joe Biden. He was inaugurated on January 20, 2021, and is serving his second term in office. 🇺🇸 \n",
            " User: \n",
            " What political party is he affiliated to?[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  AI: Great, thank you for asking! Joe Biden is affiliated with the Democratic Party. He has been a member of the Democratic Party since the early 1970s and has served as the party's nominee for President in the 2020 election. 🌊\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(user_input=\"Who was before him?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "tB92F_SknhMo",
        "outputId": "b028009e-7189-45e7-f897-0a69a348f5ab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<<SYS>>>\n",
            "You are a helpfule assistant!\n",
            "<<<SYS>>>\n",
            "\n",
            "Chat History: \n",
            " Human: Hi\n",
            "AI:   Hello! It's nice to meet you. Is there something I can help you with? 😊\n",
            "Human: Who is the president of USA?\n",
            "AI:   AI: Great, I'm glad you asked! The current President of the United States is Joe Biden. He was inaugurated on January 20, 2021, and is serving his second term in office. 🇺🇸\n",
            "Human: What political party is he affiliated to?\n",
            "AI:   AI: Great, thank you for asking! Joe Biden is affiliated with the Democratic Party. He has been a member of the Democratic Party since the early 1970s and has served as the party's nominee for President in the 2020 election. 🌊 \n",
            " User: \n",
            " Who was before him?[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  AI: Sure! Before Joe Biden, the President of the United States was Donald Trump. He served as the 45th President of the United States from 2017 to 2021. 🇺🇸'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L_GWzPHLnmxi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}